{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9879 entries, 0 to 9878\n",
      "Data columns (total 40 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   gameId                        9879 non-null   int64  \n",
      " 1   blueWins                      9879 non-null   int64  \n",
      " 2   blueWardsPlaced               9879 non-null   int64  \n",
      " 3   blueWardsDestroyed            9879 non-null   int64  \n",
      " 4   blueFirstBlood                9879 non-null   int64  \n",
      " 5   blueKills                     9879 non-null   int64  \n",
      " 6   blueDeaths                    9879 non-null   int64  \n",
      " 7   blueAssists                   9879 non-null   int64  \n",
      " 8   blueEliteMonsters             9879 non-null   int64  \n",
      " 9   blueDragons                   9879 non-null   int64  \n",
      " 10  blueHeralds                   9879 non-null   int64  \n",
      " 11  blueTowersDestroyed           9879 non-null   int64  \n",
      " 12  blueTotalGold                 9879 non-null   int64  \n",
      " 13  blueAvgLevel                  9879 non-null   float64\n",
      " 14  blueTotalExperience           9879 non-null   int64  \n",
      " 15  blueTotalMinionsKilled        9879 non-null   int64  \n",
      " 16  blueTotalJungleMinionsKilled  9879 non-null   int64  \n",
      " 17  blueGoldDiff                  9879 non-null   int64  \n",
      " 18  blueExperienceDiff            9879 non-null   int64  \n",
      " 19  blueCSPerMin                  9879 non-null   float64\n",
      " 20  blueGoldPerMin                9879 non-null   float64\n",
      " 21  redWardsPlaced                9879 non-null   int64  \n",
      " 22  redWardsDestroyed             9879 non-null   int64  \n",
      " 23  redFirstBlood                 9879 non-null   int64  \n",
      " 24  redKills                      9879 non-null   int64  \n",
      " 25  redDeaths                     9879 non-null   int64  \n",
      " 26  redAssists                    9879 non-null   int64  \n",
      " 27  redEliteMonsters              9879 non-null   int64  \n",
      " 28  redDragons                    9879 non-null   int64  \n",
      " 29  redHeralds                    9879 non-null   int64  \n",
      " 30  redTowersDestroyed            9879 non-null   int64  \n",
      " 31  redTotalGold                  9879 non-null   int64  \n",
      " 32  redAvgLevel                   9879 non-null   float64\n",
      " 33  redTotalExperience            9879 non-null   int64  \n",
      " 34  redTotalMinionsKilled         9879 non-null   int64  \n",
      " 35  redTotalJungleMinionsKilled   9879 non-null   int64  \n",
      " 36  redGoldDiff                   9879 non-null   int64  \n",
      " 37  redExperienceDiff             9879 non-null   int64  \n",
      " 38  redCSPerMin                   9879 non-null   float64\n",
      " 39  redGoldPerMin                 9879 non-null   float64\n",
      "dtypes: float64(6), int64(34)\n",
      "memory usage: 3.0 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "lol_ranked=os.path.join(os.path.pardir,'data','raw','high_diamond_ranked_10min.csv')\n",
    "df=pd.read_csv(lol_ranked)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 1.7000e+01,  2.0000e+00,  1.0000e+00, ..., -3.6180e+03,\n",
       "          1.7600e+01,  1.3670e+03],\n",
       "        [ 1.8000e+01,  1.0000e+00,  1.0000e+00, ..., -2.0580e+03,\n",
       "          2.1300e+01,  1.4153e+03],\n",
       "        [ 1.6000e+01,  6.0000e+00,  0.0000e+00, ...,  1.2850e+03,\n",
       "          2.3800e+01,  1.6230e+03],\n",
       "        ...,\n",
       "        [ 1.4000e+01,  4.0000e+00,  0.0000e+00, ..., -1.4160e+03,\n",
       "          2.1500e+01,  1.4676e+03],\n",
       "        [ 1.7000e+01,  0.0000e+00,  0.0000e+00, ...,  3.5510e+03,\n",
       "          2.4300e+01,  1.8122e+03],\n",
       "        [ 1.4000e+01,  4.0000e+00,  1.0000e+00, ...,  3.8060e+03,\n",
       "          2.1800e+01,  1.7947e+03]]),\n",
       " array([[ 1.3000e+01,  1.0000e+00,  1.0000e+00, ...,  1.8780e+03,\n",
       "          1.9700e+01,  1.6463e+03],\n",
       "        [ 1.8000e+01,  7.0000e+00,  1.0000e+00, ..., -1.7580e+03,\n",
       "          2.2500e+01,  1.4788e+03],\n",
       "        [ 1.2000e+01,  1.0000e+00,  0.0000e+00, ...,  2.1250e+03,\n",
       "          2.0600e+01,  1.8363e+03],\n",
       "        ...,\n",
       "        [ 1.5000e+01,  2.0000e+00,  1.0000e+00, ...,  3.8200e+02,\n",
       "          2.3000e+01,  1.6602e+03],\n",
       "        [ 1.7000e+01,  2.0000e+00,  1.0000e+00, ..., -2.9060e+03,\n",
       "          2.0200e+01,  1.5951e+03],\n",
       "        [ 1.4000e+01,  3.0000e+00,  0.0000e+00, ..., -1.5410e+03,\n",
       "          2.4400e+01,  1.5698e+03]]),\n",
       " array([1, 1, 0, ..., 1, 0, 0], dtype=int64),\n",
       " array([0, 1, 0, ..., 0, 1, 1], dtype=int64))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def split_input_output(df,input_one,label):\n",
    "    X=df.loc[:,input_one:].values.astype('float')\n",
    "    Y=df[label].ravel()\n",
    "    return X,Y\n",
    "def split_train_test(inputdata,outputdata):\n",
    "    X_train,X_test,y_train,y_test=train_test_split(inputdata,outputdata,test_size=0.2,random_state=0)\n",
    "    return X_train,X_test,y_train,y_test\n",
    "inputdata,outputdata=split_input_output(df,'blueWardsPlaced','blueWins')\n",
    "split_train_test(inputdata,outputdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for  model: 0.72\n",
      "precision for  model : 0.70\n",
      "recall_score for  model : 0.75\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,confusion_matrix,precision_score,recall_score\n",
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "\n",
    "def train(x_train,y_tain):\n",
    "    model_lr=LogisticRegression(random_state=0)\n",
    "    model_lr=model_lr.fit(x_train,y_train)\n",
    "    print('score for LogisticRegression model : {0:.2f}'.format(model_lr.score(x_train,y_train)))\n",
    "    #print('accuracy for baseline model: {0:.2f}'.format(accuracy_score(y_test,model_dummy.predict(X_test))))\n",
    "    #print('precision for baseline model : {0:.2f}'.format(precision_score(y_test,model_dummy.predict(X_test))))\n",
    "    #print('recall_score for baseline model : {0:.2f}'.format(recall_score(y_test,model_dummy.predict(X_test))))\n",
    "    return model\n",
    "def hyperparameter(x_train,y_tain):\n",
    "    model_lr = LogisticRegression(random_state=0)\n",
    "    parameters = {'C':[1.0,10.0,50.0,100.0,1000.0],'penalty':['l1','l2']}\n",
    "    model= GridSearchCV(model_lr,param_grid=parameters,cv=3)\n",
    "    model.fit(x_train,y_train)\n",
    "    print('score for LogisticRegression model : {0:.2f}'.format(model.score(x_train,y_train)))\n",
    "    return model\n",
    "    #print('accuracy for baseline model: {0:.2f}'.format(accuracy_score(y_test,model.predict(X_test))))\n",
    "    #print('precision for baseline model : {0:.2f}'.format(precision_score(y_test,model.predict(X_test))))\n",
    "    #print('recall_score for baseline model : {0:.2f}'.format(recall_score(y_test,model.predict(X_test))))\n",
    "def feature_standardization(iput):\n",
    "    scaler=StandardScaler()\n",
    "    input_scaled=scaler.fit_transform(iput)\n",
    "    return input_scaled\n",
    "\n",
    "def persistence(model,fileName):\n",
    "    model_file_pickle=open(fileName,'wb')\n",
    "    pickle.dump(model,model_file_pickle)\n",
    "    model_file_pickle.close()\n",
    "def load_persistence(fileName):\n",
    "    model_loaded=pickle.load(open(fileName,'rb'))\n",
    "    return model_loaded\n",
    "    \n",
    "def split_input_output(df,input_one,label):\n",
    "    X=df.loc[:,input_one:].values.astype('float')\n",
    "    Y=df[label].ravel()\n",
    "    return X,Y\n",
    "\n",
    "def split_train_test(inputdata,outputdata):\n",
    "    X_train,X_test,y_train,y_test=train_test_split(inputdata,outputdata,test_size=0.2,random_state=0)\n",
    "    return X_train,X_test,y_train,y_test\n",
    "\n",
    "def test_persistence_model(model,train,actual):\n",
    "    print('accuracy for  model: {0:.2f}'.format(accuracy_score(actual,model.predict(train))))\n",
    "    print('precision for  model : {0:.2f}'.format(precision_score(actual,model.predict(train))))\n",
    "    print('recall_score for  model : {0:.2f}'.format(recall_score(actual,model.predict(train))))\n",
    "\n",
    "if __name__ =='__main__':\n",
    "    #save model\n",
    "    #model_file_path = os.path.join(os.path.pardir,'models','2020-05-11.pkl')\n",
    "    #inputdata,outputdata=split_input_output(df,'blueWardsPlaced','blueWins')\n",
    "    #X_train,X_test,y_train,y_test=split_train_test(inputdata,outputdata)\n",
    "    #input_scaled=feature_standardization(X_train)\n",
    "    #train(X_train,y_train)\n",
    "    #model=hyperparameter(input_scaled,y_train)\n",
    "    #print(model.best_score_)\n",
    "    #persistence(model,model_file_path)\n",
    "    \n",
    "    #load model and testing score\n",
    "    #print(model_file_path)\n",
    "    model_loaded=pickle.load(open(model_file_path,'rb'))\n",
    "    #print(model_loaded)\n",
    "    model=load_persistence(model_file_path)\n",
    "    test_persistence_model(model_loaded,X_test,y_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
